{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30adf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-macosx_10_9_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp39-cp39-macosx_10_9_x86_64.whl (865 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: setuptools in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, srsly, pydantic, preshed, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: langdetect in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: emoji==1.7 in /Users/mikhailpinto/opt/anaconda3/lib/python3.9/site-packages (1.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install langdetect\n",
    "!pip install emoji==1.7\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e24de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35886d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b2a0daf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24732cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6ea8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89866ab1-54c7-40dc-8198-5aebf5f18762",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!pip install spacy\n",
    "!pip install langdetect\n",
    "!pip install emoji==1.7\n",
    "!python -m spacy download en_core_web_lg'''\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import sys\n",
    "import emoji\n",
    "import spacy\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "# Calculate coherence score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15b42afa-70e3-4ba6-84cc-34906bd8200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "        df['COMPANY'] = csv_file.split(\"/\")[-1].split('layoffs_scraped_tweets.csv')[0].capitalize()\n",
    "        df_list.append(df)\n",
    "    main_df = pd.concat(df_list, axis=0)\n",
    "    main_df.columns = ['USER', 'TWEET', 'TIMESTAMP', 'LINK', 'COMPANY']\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f0cf82b-7b05-4930-9869-489f8a09817c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87872, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('Tweets_Data/')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c5fc952-15ab-420f-b6b4-b76272df9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect the language of a sentence\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    return lang\n",
    "\n",
    "# apply the detect_language function to the TEXT column and create a new column\n",
    "df['LANGUAGE'] = df['TWEET'].apply(detect_language)\n",
    "\n",
    "# remove rows where the LANGUAGE column is not 'en' (English)\n",
    "df = df[df['LANGUAGE'] == 'en'].drop(columns=['LANGUAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "613592e8-80a0-4061-a7cc-6a17363ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27ac10a5-70fb-47ef-869a-996b37795409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "160d0668-5604-4913-a390-c4aadeff759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>LINK</th>\n",
       "      <th>COMPANY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>franchisefun</td>\n",
       "      <td>What If Franchise Business Ownership Is Not Yo...</td>\n",
       "      <td>2023-04-13 12:40:17+00:00</td>\n",
       "      <td>https://twitter.com/franchisefun/status/164649...</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>franchisefun</td>\n",
       "      <td>STOP! You Want To Start A Business With No Pri...</td>\n",
       "      <td>2023-04-12 14:38:27+00:00</td>\n",
       "      <td>https://twitter.com/franchisefun/status/164616...</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jais_george</td>\n",
       "      <td>Musk says Twitter is roughly breaking even, ha...</td>\n",
       "      <td>2023-04-12 10:00:09+00:00</td>\n",
       "      <td>https://twitter.com/jais_george/status/1646090...</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETTelecom</td>\n",
       "      <td>Musk says Twitter is roughly breaking even, ha...</td>\n",
       "      <td>2023-04-12 10:00:07+00:00</td>\n",
       "      <td>https://twitter.com/ETTelecom/status/164609081...</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>franchisefun</td>\n",
       "      <td>We Don't Know What We Don't Know!\\n\\nHow Can Y...</td>\n",
       "      <td>2023-04-11 14:23:11+00:00</td>\n",
       "      <td>https://twitter.com/franchisefun/status/164579...</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           USER                                              TWEET  \\\n",
       "0  franchisefun  What If Franchise Business Ownership Is Not Yo...   \n",
       "1  franchisefun  STOP! You Want To Start A Business With No Pri...   \n",
       "2   jais_george  Musk says Twitter is roughly breaking even, ha...   \n",
       "3     ETTelecom  Musk says Twitter is roughly breaking even, ha...   \n",
       "4  franchisefun  We Don't Know What We Don't Know!\\n\\nHow Can Y...   \n",
       "\n",
       "                   TIMESTAMP  \\\n",
       "0  2023-04-13 12:40:17+00:00   \n",
       "1  2023-04-12 14:38:27+00:00   \n",
       "2  2023-04-12 10:00:09+00:00   \n",
       "3  2023-04-12 10:00:07+00:00   \n",
       "4  2023-04-11 14:23:11+00:00   \n",
       "\n",
       "                                                LINK  COMPANY  \n",
       "0  https://twitter.com/franchisefun/status/164649...  Twitter  \n",
       "1  https://twitter.com/franchisefun/status/164616...  Twitter  \n",
       "2  https://twitter.com/jais_george/status/1646090...  Twitter  \n",
       "3  https://twitter.com/ETTelecom/status/164609081...  Twitter  \n",
       "4  https://twitter.com/franchisefun/status/164579...  Twitter  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f76b1e5-01a5-4afd-a770-eef88be8916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace emojis with their descriptions\n",
    "def replace_emojis(text):\n",
    "    for emoji_char in emoji.emojize(text).split():\n",
    "        description = emoji.demojize(emoji_char).replace(\":\",\"\")\n",
    "        text = text.replace(emoji_char, description)\n",
    "    return text\n",
    "\n",
    "def data_cleaning(main_df):\n",
    "    \n",
    "    \n",
    "    # dropping duplicates from the dataframe\n",
    "    main_df.drop_duplicates(inplace = True) \n",
    "\n",
    "    # define the regular expression pattern to match @ links\n",
    "    at_pattern = re.compile(r'@\\S+')\n",
    "    # remove http links from the TWEET column\n",
    "    \n",
    "    main_df['TWEET'].replace(at_pattern, '', regex=True, inplace = True)\n",
    "\n",
    "    # define the regular expression pattern to match http links\n",
    "    http_pattern = re.compile(r'http\\S+')\n",
    "    # remove http links from the TWEET column\n",
    "    main_df['TWEET'].replace(http_pattern, '', regex=True, inplace = True)\n",
    "\n",
    "    # define the regular expression pattern to match words with hashtags\n",
    "    hashtag_word_pattern = re.compile(r'(\\#\\w+)')\n",
    "    # extract words with hashtags from the TWEET column and place them in a new column\n",
    "    main_df['HASHTAGS'] = main_df['TWEET'].str.extractall(hashtag_word_pattern)[0].groupby(level=0).apply(list)\n",
    "\n",
    "    # remove words with hashtags from the TWEET column\n",
    "    main_df['TWEET'].replace(hashtag_word_pattern, '', regex=True, inplace = True)\n",
    "\n",
    "    # Replace emojis with their descriptions\n",
    "    main_df['TWEET'] = main_df['TWEET'].apply(lambda x: \" \".join(replace_emojis(sentence) for sentence in x.split()))\n",
    "    \n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da7bac09-4b71-43a0-899a-8ae111d4a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76179, 6)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_cleaning(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84878721-5379-4a9e-9ea3-c0315d977218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76179, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['TWEET'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bcc82de5-49fb-408e-88ae-34a10c87b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5968e90-3d5d-4214-8397-7716b1532020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44a6d810-0d0a-4d3b-a935-4aaa50abe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Preprocessing\n",
    "import nltk\n",
    "\n",
    "def preprocess_data(df, remove_stop, lemmetize,input_col_name, output_col_name):\n",
    "    \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "    df[output_col_name] = df[input_col_name].apply(lambda x: tokenizer.tokenize(x.lower()) if isinstance(x,str) else [])\n",
    "    \n",
    "    if remove_stop:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        df[output_col_name] = df[output_col_name].apply(lambda x: [w for w in x if w not in stop_words])\n",
    "    \n",
    "    df[output_col_name] = df[output_col_name].apply(lambda tokens: [token for token in tokens if len(token) > 1 and token.isalnum()])\n",
    "    \n",
    "    # code for lemmetization\n",
    "    if lemmetize:\n",
    "        pass\n",
    "    \n",
    "    df[output_col_name] = df[output_col_name].apply(lambda tokens: list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "640baacf-2752-48a9-80ff-f440d164cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(df,True,True,\"TWEET\",\"preprocessed_TWEETS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f80ceb76-4f72-45fb-b90d-62e4662e42a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [dot, connecting, another, key, franchise, goa...\n",
       "1    [stop, experience, prior, want, mad, business,...\n",
       "2    [employees, even, twitter, musk, says, breakin...\n",
       "3    [employees, even, twitter, musk, says, breakin...\n",
       "4    [need, search, franchise, business, help, know...\n",
       "Name: preprocessed_TWEETS, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"preprocessed_TWEETS\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09b08983-0407-446e-b681-7b48f327f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \n",
    "\n",
    "id2word = corpora.Dictionary(df[\"preprocessed_TWEETS\"])\n",
    "\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "corpus = [id2word.doc2bow(d) for d in df[\"preprocessed_TWEETS\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5acc1e7-e05d-4562-8ea9-2cabb4ff3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model analysis\n",
    "def LDA(num_topics, workers,passes, num_of_words, id2word_dict, corpus):\n",
    "    np.random.seed(42)\n",
    "    lda = models.ldamulticore.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word_dict,workers=workers, passes = passes)\n",
    "    return lda,lda.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f568e-7dc9-4607-997e-4166c84f1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda,topics = LDA(5,7,20,10,id2word,corpus) \n",
    "# Parameters : dataframe column, number of topics, workers for parallelization, passes (epochs ), filter_extremes, number of words per topic \n",
    "\n",
    "for topic_id, topic in topics:\n",
    "    print('Topic {}: {}'.format(topic_id, topic))\n",
    "    \n",
    "print (\"\\n\\n\")\n",
    "\n",
    "# Caluculate coherance score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df[\"preprocessed_TWEETS\"], dictionary=id2word, coherence='c_v')\n",
    "print('Coherence Score:', coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e42f1ef8-7c6f-48e7-be84-3dc9e8f07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def HDP(df_column,num_topics,num_words):\n",
    "    # Create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "    # Vectorize the data\n",
    "    X = vectorizer.fit_transform([' '.join(tokens) for tokens in df_column])\n",
    "\n",
    "    # Create a Gensim Dictionary object\n",
    "    gensim_dict = Dictionary([vectorizer.get_feature_names_out()])\n",
    "\n",
    "    # Convert the document-term matrix into a Gensim Corpus object\n",
    "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "    # Train the HDP model\n",
    "    hdp = HdpModel(corpus=corpus, id2word=gensim_dict)\n",
    "    \n",
    "    return hdp, hdp.print_topics(num_topics=num_topics, num_words=num_words),corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89829aa1-5a1c-43dd-b258-2c245f1b2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp,topics,corpus = HDP(df[\"preprocessed_TWEETS\"],5,10)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Caluculate coherance score\n",
    "coherence_model_hdp = CoherenceModel(model=hdp, texts=df[\"preprocessed_TWEETS\"], dictionary=id2word, coherence='c_v')\n",
    "print('Coherence Score:', coherence_model_hdp.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "48a2a5f5-a5de-4241-af6f-1d958f2d2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF analysis \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer;\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import normalize;\n",
    "\n",
    "def NMF_(df_column, components):\n",
    "    vectorizer = CountVectorizer(analyzer='word', max_features=10000, stop_words='english', lowercase=True)\n",
    "    x_counts = vectorizer.fit_transform([' '.join(tokens) for tokens in df_column])\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x_tfidf = transformer.fit_transform(x_counts)\n",
    "    xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "    model = NMF(n_components=components, init='nndsvd');\n",
    "    model.fit(xtfidf_norm)\n",
    "\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2543917d-d392-446d-8423-d93c1b87abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf,vectorizer=  NMF_(df[\"preprocessed_TWEETS\"],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "79a96651-d004-4074-83f0-e2c43036c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031449499299053524\n",
      "0.9985598333310292\n"
     ]
    }
   ],
   "source": [
    "topic_word_matrix = nmf.components_\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(topic_word_matrix)\n",
    "\n",
    "# Calculate the coherence score\n",
    "coherence_score = np.sum(np.triu(cosine_sim_matrix, k=1)) / (topic_word_matrix.shape[0] * (topic_word_matrix.shape[0] - 1) / 2)\n",
    "print(coherence_score)\n",
    "\n",
    "perplexity = np.exp(-nmf.reconstruction_err_ / tfidf.shape[0])\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c4d5b43d-7015-4c24-b432-5e70d9d57dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words,vectorizer):\n",
    "    \n",
    "    feat_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "652ca543-cf8b-4554-8d55-c9ec33d73cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vacancies</td>\n",
       "      <td>post</td>\n",
       "      <td>twitter</td>\n",
       "      <td>ghana</td>\n",
       "      <td>hiring</td>\n",
       "      <td>loudspeaker</td>\n",
       "      <td>job</td>\n",
       "      <td>elon</td>\n",
       "      <td>apply</td>\n",
       "      <td>employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>link</td>\n",
       "      <td>star</td>\n",
       "      <td>fired</td>\n",
       "      <td>notifications</td>\n",
       "      <td>click</td>\n",
       "      <td>calling</td>\n",
       "      <td>dream</td>\n",
       "      <td>musk</td>\n",
       "      <td>jobs</td>\n",
       "      <td>layoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>click</td>\n",
       "      <td>rocket</td>\n",
       "      <td>got</td>\n",
       "      <td>accra</td>\n",
       "      <td>houston</td>\n",
       "      <td>callingcollision</td>\n",
       "      <td>new</td>\n",
       "      <td>like</td>\n",
       "      <td>lagos</td>\n",
       "      <td>laid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apply</td>\n",
       "      <td>online</td>\n",
       "      <td>today</td>\n",
       "      <td>benefit</td>\n",
       "      <td>apply</td>\n",
       "      <td>looking</td>\n",
       "      <td>help</td>\n",
       "      <td>layoffs</td>\n",
       "      <td>location</td>\n",
       "      <td>fired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>job</td>\n",
       "      <td>resume</td>\n",
       "      <td>new</td>\n",
       "      <td>miss</td>\n",
       "      <td>2023</td>\n",
       "      <td>level</td>\n",
       "      <td>search</td>\n",
       "      <td>fires</td>\n",
       "      <td>click</td>\n",
       "      <td>meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>directly</td>\n",
       "      <td>apply</td>\n",
       "      <td>team</td>\n",
       "      <td>posts</td>\n",
       "      <td>visit</td>\n",
       "      <td>great</td>\n",
       "      <td>fed</td>\n",
       "      <td>staff</td>\n",
       "      <td>limited</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>description</td>\n",
       "      <td>pakistan</td>\n",
       "      <td>trending</td>\n",
       "      <td>remember</td>\n",
       "      <td>chicago</td>\n",
       "      <td>built</td>\n",
       "      <td>looking</td>\n",
       "      <td>changed</td>\n",
       "      <td>manager</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>visit</td>\n",
       "      <td>lahore</td>\n",
       "      <td>going</td>\n",
       "      <td>share</td>\n",
       "      <td>york</td>\n",
       "      <td>seeking</td>\n",
       "      <td>pakistan</td>\n",
       "      <td>button</td>\n",
       "      <td>officer</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bio</td>\n",
       "      <td>karachi</td>\n",
       "      <td>work</td>\n",
       "      <td>manager</td>\n",
       "      <td>los</td>\n",
       "      <td>collision</td>\n",
       "      <td>career</td>\n",
       "      <td>right</td>\n",
       "      <td>2023</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>houston</td>\n",
       "      <td>islamabad</td>\n",
       "      <td>staff</td>\n",
       "      <td>engineer</td>\n",
       "      <td>angeles</td>\n",
       "      <td>dot</td>\n",
       "      <td>postings</td>\n",
       "      <td>going</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>recruiting</td>\n",
       "      <td>rawalpindi</td>\n",
       "      <td>hq</td>\n",
       "      <td>senior</td>\n",
       "      <td>new</td>\n",
       "      <td>information</td>\n",
       "      <td>interview</td>\n",
       "      <td>day</td>\n",
       "      <td>sales</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yes</td>\n",
       "      <td>developer</td>\n",
       "      <td>right</td>\n",
       "      <td>product</td>\n",
       "      <td>hr</td>\n",
       "      <td>rate</td>\n",
       "      <td>today</td>\n",
       "      <td>firing</td>\n",
       "      <td>assistant</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comment</td>\n",
       "      <td>night</td>\n",
       "      <td>employee</td>\n",
       "      <td>associate</td>\n",
       "      <td>team</td>\n",
       "      <td>vs</td>\n",
       "      <td>check</td>\n",
       "      <td>good</td>\n",
       "      <td>visit</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>applying</td>\n",
       "      <td>executive</td>\n",
       "      <td>think</td>\n",
       "      <td>developer</td>\n",
       "      <td>work</td>\n",
       "      <td>previous</td>\n",
       "      <td>need</td>\n",
       "      <td>today</td>\n",
       "      <td>executive</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>interested</td>\n",
       "      <td>sales</td>\n",
       "      <td>layoff</td>\n",
       "      <td>compliance</td>\n",
       "      <td>join</td>\n",
       "      <td>expected</td>\n",
       "      <td>2023</td>\n",
       "      <td>people</td>\n",
       "      <td>engineer</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chicago</td>\n",
       "      <td>evening</td>\n",
       "      <td>contact</td>\n",
       "      <td>designer</td>\n",
       "      <td>companies</td>\n",
       "      <td>warning</td>\n",
       "      <td>unemployment</td>\n",
       "      <td>company</td>\n",
       "      <td>directly</td>\n",
       "      <td>layoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subscribe</td>\n",
       "      <td>manager</td>\n",
       "      <td>india</td>\n",
       "      <td>frontend</td>\n",
       "      <td>send</td>\n",
       "      <td>details</td>\n",
       "      <td>great</td>\n",
       "      <td>mass</td>\n",
       "      <td>group</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>los</td>\n",
       "      <td>marketing</td>\n",
       "      <td>working</td>\n",
       "      <td>revenue</td>\n",
       "      <td>developer</td>\n",
       "      <td>rsvp</td>\n",
       "      <td>free</td>\n",
       "      <td>real</td>\n",
       "      <td>new</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>angeles</td>\n",
       "      <td>cv</td>\n",
       "      <td>musk</td>\n",
       "      <td>controller</td>\n",
       "      <td>cv</td>\n",
       "      <td>louder</td>\n",
       "      <td>openings</td>\n",
       "      <td>bought</td>\n",
       "      <td>service</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>namibia</td>\n",
       "      <td>development</td>\n",
       "      <td>leaving</td>\n",
       "      <td>new</td>\n",
       "      <td>assistant</td>\n",
       "      <td>users</td>\n",
       "      <td>time</td>\n",
       "      <td>takeover</td>\n",
       "      <td>security</td>\n",
       "      <td>पर</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic # 01   Topic # 02 Topic # 03     Topic # 04 Topic # 05  \\\n",
       "0     vacancies         post    twitter          ghana     hiring   \n",
       "1          link         star      fired  notifications      click   \n",
       "2         click       rocket        got          accra    houston   \n",
       "3         apply       online      today        benefit      apply   \n",
       "4           job       resume        new           miss       2023   \n",
       "5      directly        apply       team          posts      visit   \n",
       "6   description     pakistan   trending       remember    chicago   \n",
       "7         visit       lahore      going          share       york   \n",
       "8           bio      karachi       work        manager        los   \n",
       "9       houston    islamabad      staff       engineer    angeles   \n",
       "10   recruiting   rawalpindi         hq         senior        new   \n",
       "11          yes    developer      right        product         hr   \n",
       "12      comment        night   employee      associate       team   \n",
       "13     applying    executive      think      developer       work   \n",
       "14   interested        sales     layoff     compliance       join   \n",
       "15      chicago      evening    contact       designer  companies   \n",
       "16    subscribe      manager      india       frontend       send   \n",
       "17          los    marketing    working        revenue  developer   \n",
       "18      angeles           cv       musk     controller         cv   \n",
       "19      namibia  development    leaving            new  assistant   \n",
       "\n",
       "          Topic # 06    Topic # 07 Topic # 08 Topic # 09 Topic # 10  \n",
       "0        loudspeaker           job       elon      apply  employees  \n",
       "1            calling         dream       musk       jobs    layoffs  \n",
       "2   callingcollision           new       like      lagos       laid  \n",
       "3            looking          help    layoffs   location      fired  \n",
       "4              level        search      fires      click       meta  \n",
       "5              great           fed      staff    limited     amazon  \n",
       "6              built       looking    changed    manager     google  \n",
       "7            seeking      pakistan     button    officer       like  \n",
       "8          collision        career      right       2023    company  \n",
       "9                dot      postings      going    nigeria     people  \n",
       "10       information     interview        day      sales       read  \n",
       "11              rate         today     firing  assistant       good  \n",
       "12                vs         check       good      visit       tech  \n",
       "13          previous          need      today  executive      today  \n",
       "14          expected          2023     people   engineer       know  \n",
       "15           warning  unemployment    company   directly     layoff  \n",
       "16           details         great       mass      group      india  \n",
       "17              rsvp          free       real        new       work  \n",
       "18            louder      openings     bought    service      right  \n",
       "19             users          time   takeover   security         पर  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "nmf_df = get_nmf_topics(nmf, 5,vectorizer)\n",
    "nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383e0ec-f9ab-4b41-83a0-a8ce255b5efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
